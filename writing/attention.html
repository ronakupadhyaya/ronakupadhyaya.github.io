
<!DOCTYPE html>
<html>
<head>
  <title>Ronak Upadhyaya</title>
  <style>
  * {
    font-family:'Alegreya Sans', 'Helvetica Neue', Helvetica, Arial, sans-serif;
  }
  </style>
</head>
<body>
  <div style="display:flex;flex-direction:column;margin:auto;width:50%">
    <div style="display:flex;flex-direction:row;font-size:14px;justify-content:space-between;">
      <div>
          <p>
            <a href="../index.html" style="text-decoration: none; color:#2f2f2f">Ronak Upadhyaya</a>
          </p>
      </div>
      <ul style="display:flex;list-style-type:none">
        <li><a href="../index.html" style="margin-left:10px;margin-right:10px;text-decoration:none;color:#2f2f2f">About</a></li>
        <li><a href="../writing.html" style="margin-left:10px;margin-right:10px;text-decoration:none;color:#2f2f2f">Writing</a></li>
        <li><a href="../reading.html" style="margin-left:10px;margin-right:10px;text-decoration:none;color:#2f2f2f">Reading</a></li>
        <!-- <li><a href="../projects.html" style="margin-left:10px;margin-right:10px;text-decoration:none;color:#2f2f2f">Projects</a></li> -->
        <li><a href="../interests.html" style="margin-left:10px;margin-right:10px;text-decoration:none;color:#2f2f2f">Interests</a></li>
      </ul>
    </div>
    <div style="display:flex;flex-direction:column;margin-bottom:40px">
      <div style="margin-top:40px;font-weight:bold;font-size:25px">
          TRANSFORMERS, ATTENTION, AND PHILOSOPHY OF LANGUAGE
      </div>
      <div style="font-size:16px;margin-top:20px;line-height:1.7;color:#2f2f2f">
        LLMs represent a fundamental advance, demonstrating the capacity to generate coherent and contextually appropriate text. They are based on the <a href="https://arxiv.org/pdf/1706.03762" target="_blank">transformer architecture</a>, which leverages the attention mechanism to dynamically weight the relevance of different tokens within a given context. The emergence of transformers has profound implications for the philosophy of language, particularly for long-standing debates regarding whether meaning is intrinsic or derived from contextual usage.
      </div>
      <div style="margin-top:20px;margin-left:auto;margin-right:auto;">
        <img style="margin:auto;height:240px;width:187px" src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/ab/Ludwig_Wittgenstein.jpg/819px-Ludwig_Wittgenstein.jpg" />
        <figcaption style="text-align:center;font-size:14px;line-height:1.7;color:#2f2f2f">Ludwig Wittgenstein</figcaption>
      </div>
      <div style="font-size:16px;margin-top:20px;line-height:1.7;color:#2f2f2f">
        The ability of attention-based models to acquire sophisticated linguistic competence purely through statistical learning over text raises questions about the role of innate linguistic structures, the necessity of truth-conditional meaning, and the extent to which formal rules underlie language comprehension. At the same time, one must recognize that how AI models learn language might not directly translate to how humans acquire it. Instead, their success provides a constructive proof that alternative pathways to language competence are possible.
      </div>
      <div style="margin-top:30px;font-size:20px;font-weight:bold;">
        Competing Views on Language
      </div>
      <div style="font-size:16px;margin-top:20px;line-height:1.7;color:#2f2f2f">
        Throughout history, attempts to understand language have given rise to several theoretical frameworks. While these theories differ, they can be grouped into four overarching perspectives: structural theories, formal and logical semantics, usage-based and distributional theories, and nativist approaches.
      </div>
      <div style="font-size:16px;margin-top:20px;line-height:1.7;color:#2f2f2f">
        <a href="https://en.wikipedia.org/wiki/Structural_linguistics" target="_blank">Structural theories</a>, as pioneered by Ferdinand de Saussure, view language as a system of interrelated signs where meaning arises from differences between elements rather than intrinsic properties. This perspective treats language as a structured whole, in which relationships among elements determine meaning.
      </div>
      <div style="font-size:16px;margin-top:20px;line-height:1.7;color:#2f2f2f">
        <a href="https://en.wikipedia.org/wiki/Mediated_reference_theory" target="_blank">Logical semanticists</a> such as Gottlob Frege and Bertrand Russell, and later Rudolf Carnap, sought to ground meaning in truth conditions and formal rules, aiming to define a mathematically precise foundation for language. Carnap argued that a formal system should make "no reference to the meaning of the symbols... but simply and solely to the kinds and order of the symbols from which the expressions are constructed." These theories assume that meaning is inherently structured and governed by logical and syntactic constraints rather than emerging dynamically from usage.
      </div>
      <div style="font-size:16px;margin-top:20px;line-height:1.7;color:#2f2f2f">
        Usage-based theories, drawing on the ideas of Wittgenstein’s "<a href="https://en.wikipedia.org/wiki/Language_game_(philosophy)" target="_blank">language games</a>" and J.R. Firth’s maxim, “You shall know a word by the company it keeps,” propose that meaning arises from patterns of use rather than being fixed or intrinsic. Empirical support for this view is found in Zellig Harris’s work, which demonstrated that linguistic structure could be inferred from word co-occurrence patterns.
      </div>
      <div style="font-size:16px;margin-top:20px;line-height:1.7;color:#2f2f2f">
        Nativist theories propose that linguistic competence is an innate human faculty, rather than something that emerges solely from experience. Noam Chomsky’s <a href="https://en.wikipedia.org/wiki/Universal_grammar" target="_blank">Universal Grammar (UG)</a> argued that humans possess an inborn "language acquisition device" containing fundamental grammatical principles. This view sought to explain how children acquire language despite limited exposure, emphasizing the presence of deep syntactic structures that constrain possible grammars. 
      </div>
      <div style="margin-top:30px;font-size:20px;font-weight:bold;">
        Early Computational Approaches and Their Limitations
      </div>
      <div style="font-size:16px;margin-top:20px;line-height:1.7;color:#2f2f2f">
        The pursuit of natural language understanding has long been a central goal in artificial intelligence. Early approaches, such as Terry Winograd’s <a href="https://en.wikipedia.org/wiki/SHRDLU" target="_blank">SHRDLU</a>, attempted to encode explicit grammatical and semantic rules. It could process language within a constrained micro-world, but it was brittle and unable to scale to the flexibility and ambiguity of real-world language. SHRDLU's reliance on handcrafted rules made it impractical for capturing the fluidity of human communication.
      </div>
      <div style="font-size:16px;margin-top:20px;line-height:1.7;color:#2f2f2f">
        Recurrent Neural Networks (RNNs) represented a step forward, as they processed language dynamically rather than relying on predefined rules. By maintaining a representation of prior words in a sequence, RNNs could model simple dependencies. However, they struggled with long-range dependencies, often failing to retain information from earlier parts of a sentence when processing later ones. The introduction of Long Short-Term Memory (LSTM) networks improved upon this by allowing models to maintain memory over longer sequences. However, even LSTMs were limited in their ability to capture complex linguistic relationships, frequently forgetting earlier words when processing later ones.
      </div>
      <div style="margin-top:30px;font-size:20px;font-weight:bold;">
        Transformers and the Attention Mechanism
      </div>
      <div style="font-size:16px;margin-top:20px;line-height:1.7;color:#2f2f2f">
        The transformer architecture revolutionized natural language processing by removing sequential dependencies and replacing them with a mechanism that evaluates all words in a sentence simultaneously. The transformer consists of self-attention mechanisms, which allow the model to assign dynamic importance to different words in a given context. Unlike RNNs, which process words in order, transformers consider all words at once, making them far more effective at capturing long-range dependencies.
      </div>
      <div style="font-size:16px;margin-top:20px;line-height:1.7;color:#2f2f2f">
        The attention mechanism is particularly noteworthy because it mirrors human cognitive strategies for language comprehension. Humans do not process sentences in a strictly linear fashion but instead shift focus dynamically depending on meaning. For example, when reading "The bank approved the loan despite the risks," our interpretation of "bank" depends on the surrounding context. Transformers operate similarly, adjusting how much weight they assign to each token based on its relevance within a given passage. This ability to infer meaning dynamically, rather than relying on rigid rules, aligns with the distributional hypothesis that meaning emerges from use.
      </div>
      <div style="margin-top:30px;font-size:20px;font-weight:bold;">
        Structural Foundations
      </div>
      <div style="font-size:16px;margin-top:20px;line-height:1.7;color:#2f2f2f">
        LLMs inherently capture complex inter-word relationships that align with the principles of structural theories. By learning distributed representations that encode the differences and relations between tokens, transformers mirror Saussure’s idea that meaning arises from the network of differences among signs. Although these models do not explicitly represent the signifier/signified dichotomy, the emergent relational patterns in their internal representations provide a computational demonstration of structural insights.
      </div>
      <div style="margin-top:30px;font-size:20px;font-weight:bold;">
        Reevaluating Formal Semantics
      </div>
      <div style="font-size:16px;margin-top:20px;line-height:1.7;color:#2f2f2f">
        LLMs generate coherent and grammatically correct sentences by absorbing patterns from vast amounts of text. This emergent behavior challenges the necessity for pre-specified, truth-conditional rules as championed by formal semanticists like Frege, Russell, and Carnap. While LLMs capture aspects of formal structure, they do so without an explicit grounding in external truth conditions. Thus, the success of transformers raises questions about whether a purely formal account of meaning is sufficient.
      </div>
      <div style="margin-top:30px;font-size:20px;font-weight:bold;">
        Embracing Usage-Based Views
      </div>
      <div style="font-size:16px;margin-top:20px;line-height:1.7;color:#2f2f2f">
        The transformer’s dynamic self-attention mechanism exemplifies the idea that meaning emerges from usage. By continuously adjusting the weight of each token based on its context, transformers operationalize the notion central to Wittgenstein’s language games and Firth’s distributional hypothesis. LLMs validate that sophisticated linguistic competence can be acquired solely from patterns of word co-occurrence and context, reinforcing the view that semantic understanding need not depend on fixed, intrinsic meanings.
      </div>
      <div style="margin-top:30px;font-size:20px;font-weight:bold;">
        Rethinking Nativist Assumptions
      </div>
      <div style="font-size:16px;margin-top:20px;line-height:1.7;color:#2f2f2f">
        The impressive performance of LLMs in learning syntax and generating contextually appropriate text challenges the traditional nativist claim that innate mechanisms, such as Chomsky’s Universal Grammar, are indispensable for language acquisition. Although these models are trained on data volumes far exceeding what human children typically encounter, their success suggests that many aspects of grammatical structure might emerge from exposure and general learning processes rather than relying solely on pre-wired cognitive frameworks.
      </div>
      </div>
      </div>
    </div>
  </div>
</body>
</html>
